## Escribir las reglas de análisis:

### 1.- Utilizar tu tabla de tokens

Antes de escribir las reglas de análisis, necesitas tener tu lista de los tokens
que tu analizador léxico debe reconocer en el texto de entrada. Estos tokens pueden incluir palabras clave,
identificadores, números, operadores, símbolos especiales, etc.

### Tabla de tokens

| Token            | Expresión Regular    |
| ---------------- | -------------------- |
| IDENTIFIER       | [a-zA-Z][a-zA-Z0-9_] |
| PLUS             | \\\+                 |
| MINUS            | \\\-                 |
| TIMES            | \\\*                 |
| DIVIDE           | /                    |
| MODULO           | %                    |
| POWER            | \*\*                 |
| ASSIGN_OP        | :=                   |
| TYPE_DECLARATION | :                    |
| COMPARISON       | =                    |
| LESS_THAN        | <                    |
| GREATER_THAN     | >                    |
| GREATER_EQUAL    | >=                   |
| LESS_EQUAL       | <=                   |
| NOT_EQUAL        | !=                   |
| NOT              | !                    |
| AND              | &&                   |
| OR               | \\\|\\\|             |
| COMMENT_START    | \/\\?                |
| COMMENT_END      | \\?\/                |
| LEFT_PAREN       | \\\(                 |
| RIGHT_PAREN      | \\\)                 |
| LEFT_BRACKET     | \\\[                 |
| RIGHT_BRACKET    | \\\]                 |
| LEFT_BRACE       | \\\{                 |
| RIGHT_BRACE      | \\\}                 |
| SEMICOLO         | ;                    |

### Palabras reservadas

| Token    | Expresión Regular |
| -------- | ----------------- |
| FUNCTION | fun               |
| RETURNS  | ->                |
| IF       | if                |
| ELSE     | else              |
| FOR      | for               |
| IN       | in                |
| WHILE    | while             |

### Tipos de datos

| Token        | Expresión Regular |
| ------------ | ----------------- |
| TYPE_INTEGER | int               |
| TYPE_STRING  | str               |
| TYPE_FLOAT   | float             |
| TYPE_DOUBLE  | double            |
| TYPE_BOOL    | bool              |
| TYPE_VOID    | void              |
| TYPE_NULL    | null              |

### Literales de tipos de datos

| Token  | Expresión Regular |
| ------ | ----------------- |
| STRING | "[^\s\"]"         |
| INT    | [0-9]+            |
| FLOAT  | [0-9]+\.[0-9]+f   |
| DOUBLE | [0-9]+\.[0-9]+    |
| BOOL   | (true\|false)     |

### 2.- Escribir las reglas de coincidencia:

Utilizando la sintaxis proporcionada por la herramienta o librería
seleccionada, escribe las reglas de coincidencia que definan cómo se reconocen los tokens en el texto de entrada.
Estas reglas suelen estar escritas en forma de expresiones regulares o reglas de coincidencia de patrones.

#### Nuestras reglas de coincidencia:

#### Para operadores matemáticos

t_PLUS = r'\+'
t_MINUS = r'-'
t_TIMES = r'\*'
t_DIVIDE = r'/'
t_MODULO = r'%'

#### Para operadores de asignación y comparación

t_ASSIGN_OP = r':='
t_ASSIGN_OP_SHORT = r':'
t_COMPARISON = r'='

#### Para operadores de comparación

t_LESS_THAN = r'<'
t_GREATER_THAN = r'>'
t_GREATER_EQUAL = r'>='
t_LESS_EQUAL = r'<='
t_NOT_EQUAL = r'!='

#### Para operadores lógicos

t_NOT = r'!'
t_AND = r'&&'
t_OR = r'\|\|'

#### Para paréntesis, corchetes y llaves

t_LEFT_PAREN = r'\('
t_RIGHT_PAREN = r'\)'
t_LEFT_BRACKET = r'\['
t_RIGHT_BRACKET = r'\]'
t_LEFT_BRACE = r'\{'
t_RIGHT_BRACE = r'\}'

### 3.- Asociar acciones a las reglas:

Además de definir las reglas de coincidencia, también puedes asociar acciones a cada
regla. Estas acciones se ejecutan cuando se reconoce un token según la regla correspondiente. Las acciones pueden
incluir la asignación de un tipo de token, la recopilación de información adicional, la generación de mensajes de
error, etc.

def t_RETURNS(t):
r'returns'
return t

def t_INTEGER(t):
r'\d+'
t.value = int(t.value) # Convertir el valor del token a un entero
return t

def t_STRING(t):
r'\'[^\']\*\''
t.value = t.value[1:-1] # Eliminar las comillas del principio y del final
return t

def t_FUNCTION(t):
r'fun'
return t

def t_IF(t):
r'if'
return t

def t_ELSE(t):
r'else'
return t

def t_FOR(t):
r'for'
return t

def t_WHILE(t):
r'while'
return t

def t*IDENTIFIER(t):
r'[a-zA-Z*][a-zA-Z0-9_]\*'
return t

def t_COMMENT(t):
r'\?.\*'
pass

def t_SPACE(t):
r'\t+'
pass

def t_NEWLINE(t):
r'\n+'
t.lexer.lineno += len(t.value)

def t_error(t):
global resultado_lexema
estado = "\*\* El token no es válido en la línea {:4} Valor {:16} Posición {:4}".format(str(t.lineno), str(t.value), str(t.lexpos))
resultado_lexema.append(estado)
t.lexer.skip(1)

### 4.- Manejo de casos especiales:

Considera cómo manejar los casos especiales o ambigüedades en las reglas de análisis.
Por ejemplo, si un mismo patrón puede corresponder a múltiples tokens, debes definir reglas que especifiquen cómo
resolver estas ambigüedades.

### 5.- Pruebas de las reglas:

Después de escribir las reglas de análisis, pruébalas utilizando una variedad de ejemplos
de texto de entrada para asegurarte de que funcionen correctamente. Ajusta las reglas según sea necesario para
corregir cualquier error o comportamiento inesperado que encuentres durante las pruebas.

### 6.- Optimización (opcional):

Si es necesario, puedes optimizar las reglas de análisis para mejorar el rendimiento de
tu analizador léxico. Esto puede incluir la reorganización de las reglas para minimizar la cantidad de comparaciones
necesarias o el uso de técnicas avanzadas de coincidencia de patrones.
